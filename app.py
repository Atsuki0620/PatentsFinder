import os
import json
import yaml
import pandas as pd
import streamlit as st
from google.oauth2 import service_account
from src.utils.patent_utils import PatentSearchUtils

# â”€â”€â”€ Secrets å–å¾— â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
sa_info = st.secrets.get("GCP_SERVICE_ACCOUNT") or os.getenv("GCP_SERVICE_ACCOUNT")
if not sa_info:
    st.error("GCP ã®ã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆæƒ…å ±ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
    st.stop()
credentials = service_account.Credentials.from_service_account_info(json.loads(sa_info))

openai_key = st.secrets.get("OPENAI_API_KEY") or os.getenv("OPENAI_API_KEY")
if not openai_key:
    st.error("OpenAI API Key ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
    st.stop()

# â”€â”€â”€ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒªã‚½ãƒ¼ã‚¹å®šç¾© â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@st.cache_resource
def load_config() -> dict:
    with open("config/config.yaml", "r", encoding="utf-8") as f:
        return yaml.safe_load(f)

@st.cache_resource
def get_utils() -> PatentSearchUtils:
    cfg = load_config()
    return PatentSearchUtils(
        config=cfg,
        credentials=credentials,
        openai_api_key=openai_key
    )

# â”€â”€â”€ è¨­å®šã¨ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆæº–å‚™ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
config = load_config()
utils = get_utils()
openai_client = utils.openai_client
llm_model = utils.llm_model

initial_prompt = config["chat_flow"]["initial_prompt"]
proposal_prompt = config["chat_flow"]["proposal_prompt"]

# â”€â”€â”€ ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆåˆæœŸåŒ– â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if "mode" not in st.session_state:
    st.session_state.mode = "question"
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []
if "awaiting_confirm" not in st.session_state:
    st.session_state.awaiting_confirm = False
if "initial_prompt_shown" not in st.session_state:
    st.session_state.initial_prompt_shown = False

# â”€â”€â”€ ãƒãƒ£ãƒƒãƒˆæç”»ãƒ˜ãƒ«ãƒ‘ãƒ¼ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def render_chat():
    for msg in st.session_state.chat_history:
        role = "assistant" if msg["role"] in ("assistant", "system") else "user"
        st.chat_message(role).write(msg["content"])

# â”€â”€â”€ è³ªå•ãƒ•ã‚§ãƒ¼ã‚º â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def question_phase():
    # åˆå›ã¯ä¸€åº¦ã ã‘åˆæœŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’è¡¨ç¤º
    if not st.session_state.initial_prompt_shown:
        st.session_state.chat_history.append({
            "role": "assistant",
            "content": initial_prompt
        })
        st.session_state.initial_prompt_shown = True

    render_chat()

    if not st.session_state.awaiting_confirm:
        user_input = st.chat_input("è‡ªç”±ã«å…¥åŠ›ã—ã¦ãã ã•ã„â€¦")
        if user_input:
            st.session_state.chat_history.append({
                "role": "user",
                "content": user_input
            })
            render_chat()

            # è§£é‡ˆç¢ºèªç”¨ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ç”Ÿæˆ
            resp = openai_client.chat.completions.create(
                model=llm_model,
                messages=[
                    {
                        "role": "system",
                        "content": "ä»¥ä¸‹ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ç™ºè¨€ã‚’è¦ç´„ã—ã€ã€Œã“ã†ã„ã†æ„å‘³ã§ãŠé–“é•ã„ãªã„ã§ã™ã‹ï¼Ÿã€ã¨ã„ã†è‡ªç„¶æ–‡ã§ç¢ºèªãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚"
                    },
                    {"role": "user", "content": user_input}
                ],
                temperature=0
            )
            interpretation = resp.choices[0].message.content.strip()

            st.session_state.chat_history.append({
                "role": "assistant",
                "content": interpretation
            })
            st.session_state.awaiting_confirm = True
            render_chat()

    else:
        confirm = st.chat_input("ã“ã®ç†è§£ã§ã‚ˆã‚ã—ã„ã§ã™ã‹ï¼Ÿã€Œã¯ã„ã€ã¾ãŸã¯ã€Œã„ã„ãˆã€ã§ã”å›ç­”ãã ã•ã„ã€‚")
        if confirm:
            st.session_state.chat_history.append({
                "role": "user",
                "content": confirm
            })
            render_chat()

            if confirm.lower() in ["ã¯ã„", "yes"]:
                st.session_state.mode = "proposal"
            else:
                # èª¤è§£æ™‚ã¯å±¥æ­´ã‚’æ®‹ã—ã¦å†å…¥åŠ›ã‚’ä¿ƒã™
                st.session_state.chat_history.append({
                    "role": "assistant",
                    "content": "ã™ã¿ã¾ã›ã‚“ã€èª¤è§£ãŒã‚ã£ãŸã‚ˆã†ã§ã™ã€‚ã‚‚ã†ä¸€åº¦æ•™ãˆã¦ãã ã•ã„ï¼"
                })
                render_chat()

            st.session_state.awaiting_confirm = False

# â”€â”€â”€ ææ¡ˆãƒ•ã‚§ãƒ¼ã‚º â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def proposal_phase():
    render_chat()

    if "proposal" not in st.session_state:
        resp = openai_client.chat.completions.create(
            model=llm_model,
            messages=[
                {"role": "system", "content": proposal_prompt},
                *[
                    {"role": m["role"], "content": m["content"]}
                    for m in st.session_state.chat_history
                    if m["role"] == "user"
                ]
            ],
            temperature=0
        )
        st.session_state.proposal = resp.choices[0].message.content.strip()

    st.chat_message("assistant").write("ã“ã¡ã‚‰ã®æ¤œç´¢æ–¹é‡ã§ã‚ˆã‚ã—ã„ã§ã—ã‚‡ã†ã‹ï¼Ÿ")
    st.code(st.session_state.proposal, language="json")

    col1, col2 = st.columns(2)
    if col1.button("æ¤œç´¢å®Ÿè¡Œ"):
        st.session_state.mode = "execute"
    if col2.button("ä¿®æ­£ã™ã‚‹"):
        # ã€Œä¿®æ­£ã™ã‚‹ã€æ™‚ã ã‘å±¥æ­´ã¨ãƒ•ãƒ©ã‚°ã‚’ãƒªã‚»ãƒƒãƒˆ
        st.session_state.chat_history = []
        st.session_state.initial_prompt_shown = False
        st.session_state.proposal = None
        st.session_state.mode = "question"

# â”€â”€â”€ å®Ÿè¡Œãƒ•ã‚§ãƒ¼ã‚º â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def execute_phase():
    render_chat()
    params = json.loads(st.session_state.proposal)
    query = utils.build_query(params)
    df = utils.search_patents(query)

    st.chat_message("assistant").write(f"ğŸ” æ¤œç´¢çµæœï¼š{len(df)} ä»¶ã§ã™ï¼")
    st.dataframe(df)
    csv_data = df.to_csv(index=False).encode("utf-8-sig")
    st.download_button("CSV ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰", csv_data, "results.csv", "text/csv")

# â”€â”€â”€ ãƒ¡ã‚¤ãƒ³ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.title("ğŸ” ç‰¹è¨±èª¿æŸ»æ”¯æ´ã‚·ã‚¹ãƒ†ãƒ ï¼ˆãƒãƒ£ãƒƒãƒˆUIç‰ˆï¼‰")

if st.session_state.mode == "question":
    question_phase()
elif st.session_state.mode == "proposal":
    proposal_phase()
else:
    execute_phase()
