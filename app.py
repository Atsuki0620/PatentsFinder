import os
import json
import yaml
import streamlit as st
from google.oauth2 import service_account
from src.utils.patent_utils import PatentSearchUtils

# --- è³ªå•ãƒ•ãƒ­ãƒ¼ç”¨è¨­å®šãƒ­ãƒ¼ãƒ‰ ---
@st.cache_resource
def load_config():
    with open("config/config.yaml", "r", encoding="utf-8") as f:
        return yaml.safe_load(f)

cfg = load_config()
questions = cfg['chat_flow']['questions']  # è³ªå•ãƒªã‚¹ãƒˆ

# --- ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆåˆæœŸåŒ– ---
if 'mode' not in st.session_state:
    st.session_state.mode = 'question'      # question, proposal, execute
    st.session_state.step = 0               # ç¾åœ¨ã®è³ªå•ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
    st.session_state.chat_history = []      # ä¼šè©±å±¥æ­´
    st.session_state.proposal = None        # æ¤œç´¢æ–¹é‡

# --- Streamlit Secrets ---
sa_info = st.secrets.get("GCP_SERVICE_ACCOUNT") or os.getenv("GCP_SERVICE_ACCOUNT")
credentials = service_account.Credentials.from_service_account_info(json.loads(sa_info))
openai_key = st.secrets.get("OPENAI_API_KEY") or os.getenv("OPENAI_API_KEY")

@st.cache_resource
def get_utils():
    return PatentSearchUtils(
        config=cfg,
        credentials=credentials,
        openai_api_key=openai_key
    )

# --- ãƒ•ã‚§ãƒ¼ã‚ºã”ã¨ã® UI ---
def question_phase():
    q = questions[st.session_state.step]
    st.markdown(f"**Q: {q['text']}**")
    answer = st.text_input("å›ç­”ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„", key=f"ans_{st.session_state.step}")
    if st.button("é€ä¿¡", key=f"send_{st.session_state.step}"):
        st.session_state.chat_history.append({'role': 'user', 'content': answer})
        st.session_state.chat_history.append({'role': 'system', 'content': q['followup_prompt'].format(answer)})
        st.session_state.step += 1
        if st.session_state.step >= len(questions):
            st.session_state.mode = 'proposal'
        st.experimental_rerun()


def proposal_phase():
    utils = get_utils()
    # ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
    prompt = cfg['chat_flow']['proposal_prompt']
    msgs = [{'role':'system','content': prompt}] + st.session_state.chat_history
    resp = utils.openai_client.chat.completions.create(
        model=utils.llm_model,
        messages=msgs,
        temperature=0
    )
    proposal = resp.choices[0].message.content.strip()
    st.session_state.proposal = proposal
    st.markdown("**ææ¡ˆã•ã‚ŒãŸæ¤œç´¢æ–¹é‡**")
    st.write(proposal)
    col1, col2 = st.columns(2)
    if col1.button("æ¤œç´¢å®Ÿè¡Œ"):
        st.session_state.mode = 'execute'
        st.experimental_rerun()
    if col2.button("ä¿®æ­£ã™ã‚‹"):
        st.session_state.mode = 'question'
        st.session_state.step = 0
        st.session_state.chat_history = []
        st.experimental_rerun()


def execute_phase():
    utils = get_utils()
    # ã“ã“ã§ proposal ã‹ã‚‰ JSON ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æŠ½å‡º or build_query å®Ÿè¡Œ
    params = json.loads(st.session_state.proposal)  # proposal ã« JSON ãŒå«ã¾ã‚Œã¦ã„ã‚‹æƒ³å®š
    sql = utils.build_query(params)
    df = utils.search_patents(sql)
    st.subheader(f"ğŸ” æ¤œç´¢çµæœ: {len(df)} ä»¶")
    st.dataframe(df)

# --- ãƒ¡ã‚¤ãƒ³ ---
st.title("ğŸ” ç‰¹è¨±èª¿æŸ»æ”¯æ´ã‚·ã‚¹ãƒ†ãƒ ï¼ˆãƒãƒ£ãƒƒãƒˆãƒ•ãƒ­ãƒ¼ç‰ˆï¼‰")
if st.session_state.mode == 'question':
    question_phase()
elif st.session_state.mode == 'proposal':
    proposal_phase()
else:
    execute_phase()
