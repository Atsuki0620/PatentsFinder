import os
import json
import yaml
import pandas as pd
import streamlit as st
from google.oauth2 import service_account
from src.utils.patent_utils import PatentSearchUtils

# â”€â”€â”€ Secrets å–å¾— â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
sa_info = st.secrets.get("GCP_SERVICE_ACCOUNT") or os.getenv("GCP_SERVICE_ACCOUNT")
if not sa_info:
    st.error("GCP ã®ã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆæƒ…å ±ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
    st.stop()
credentials = service_account.Credentials.from_service_account_info(json.loads(sa_info))

openai_key = st.secrets.get("OPENAI_API_KEY") or os.getenv("OPENAI_API_KEY")
if not openai_key:
    st.error("OpenAI API Key ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
    st.stop()

# â”€â”€â”€ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒªã‚½ãƒ¼ã‚¹å®šç¾© â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@st.cache_resource
def load_config() -> dict:
    with open("config/config.yaml", "r", encoding="utf-8") as f:
        return yaml.safe_load(f)

@st.cache_resource
def get_utils() -> PatentSearchUtils:
    cfg = load_config()
    return PatentSearchUtils(
        config=cfg,
        credentials=credentials,
        openai_api_key=openai_key
    )

config = load_config()
utils = get_utils()
openai_client = utils.openai_client
llm_model = utils.llm_model

# ãƒãƒ£ãƒƒãƒˆãƒ•ãƒ­ãƒ¼ç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
initial_prompt = config["chat_flow"]["initial_prompt"]
proposal_prompt = config["chat_flow"]["proposal_prompt"]

# â”€â”€â”€ ãƒãƒ£ãƒƒãƒˆæç”»ãƒ˜ãƒ«ãƒ‘ãƒ¼ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def render_chat():
    for msg in st.session_state.chat_history:
        if msg["role"] == "assistant" or msg["role"] == "system":
            st.chat_message("assistant").write(msg["content"])
        else:
            st.chat_message("user").write(msg["content"])

# â”€â”€â”€ è³ªå•ãƒ•ã‚§ãƒ¼ã‚º â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def question_phase():
    # åˆæœŸåŒ–
    if "mode" not in st.session_state:
        st.session_state.mode = "question"
        st.session_state.chat_history = []
        st.session_state.awaiting_confirm = False

    # åˆå›ã ã‘åˆæœŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’è¡¨ç¤º
    if not st.session_state.chat_history:
        st.session_state.chat_history.append({
            "role": "system",
            "content": initial_prompt
        })

    render_chat()

    # ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ› or ç¢ºèªãƒ¢ãƒ¼ãƒ‰ã‹ã§å‡¦ç†ã‚’åˆ†å²
    if not st.session_state.awaiting_confirm:
        user_input = st.chat_input("è‡ªç”±ã«å…¥åŠ›ã—ã¦ãã ã•ã„â€¦")
        if user_input:
            # ãƒ¦ãƒ¼ã‚¶ãƒ¼ç™ºè¨€ã‚’å±¥æ­´ã«
            st.session_state.chat_history.append({
                "role": "user",
                "content": user_input
            })
            render_chat()

            # è§£é‡ˆç¢ºèªç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½œæˆã—ã¦ LLM ã«æŠ•ã’ã‚‹
            resp = openai_client.chat.completions.create(
                model=llm_model,
                messages=[
                    {
                        "role": "system",
                        "content": "ä»¥ä¸‹ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ç™ºè¨€ã‚’è¦ç´„ã—ã€ã€Œã“ã†ã„ã†æ„å‘³ã§ãŠé–“é•ã„ãªã„ã§ã™ã‹ï¼Ÿã€ã¨ã„ã†å½¢å¼ã§ç¢ºèªæ–‡ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚"
                    },
                    {"role": "user", "content": user_input}
                ],
                temperature=0
            )
            interpretation = resp.choices[0].message.content.strip()

            # è§£é‡ˆçµæœã‚’ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆç™ºè¨€ã¨ã—ã¦è¿½åŠ 
            st.session_state.chat_history.append({
                "role": "assistant",
                "content": interpretation
            })
            st.session_state.awaiting_confirm = True
            render_chat()

    else:
        # è§£é‡ˆç¢ºèªã®ãŸã‚ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ã‚’å¾…ã¤
        confirm = st.chat_input("ã“ã®ç†è§£ã§ã‚ˆã‚ã—ã„ã§ã™ã‹ï¼Ÿã€Œã¯ã„ã€ã¾ãŸã¯ã€Œã„ã„ãˆã€ã§ã”å›ç­”ãã ã•ã„ã€‚")
        if confirm:
            st.session_state.chat_history.append({
                "role": "user",
                "content": confirm
            })
            render_chat()

            if confirm.lower() in ["ã¯ã„", "yes"]:
                st.session_state.mode = "proposal"
            else:
                # èª¤è§£ãŒã‚ã£ãŸå ´åˆã€å†å…¥åŠ›ã‚’ä¿ƒã™
                st.session_state.chat_history.append({
                    "role": "assistant",
                    "content": "ã™ã¿ã¾ã›ã‚“ã€èª¤è§£ãŒã‚ã£ãŸã‚ˆã†ã§ã™ã€‚ã‚‚ã†ä¸€åº¦æ•™ãˆã¦ãã ã•ã„ï¼"
                })
                render_chat()
                # stay in question mode

            st.session_state.awaiting_confirm = False

# â”€â”€â”€ ææ¡ˆãƒ•ã‚§ãƒ¼ã‚º â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def proposal_phase():
    render_chat()
    if "proposal" not in st.session_state:
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¨ã®ã‚„ã‚Šå–ã‚Šã‚’å…ƒã«æ¤œç´¢ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ JSON ã‚’ç”Ÿæˆ
        resp = openai_client.chat.completions.create(
            model=llm_model,
            messages=[
                {"role": "system", "content": proposal_prompt},
                *[
                    {"role": m["role"], "content": m["content"]}
                    for m in st.session_state.chat_history
                    if m["role"] in ("user",)
                ]
            ],
            temperature=0
        )
        proposal = resp.choices[0].message.content.strip()
        st.session_state.proposal = proposal

    # ææ¡ˆã‚’è¡¨ç¤º
    st.chat_message("assistant").write("ã“ã¡ã‚‰ã®æ¤œç´¢æ–¹é‡ã§ã‚ˆã‚ã—ã„ã§ã—ã‚‡ã†ã‹ï¼Ÿ")
    st.code(st.session_state.proposal, language="json")

    col1, col2 = st.columns(2)
    if col1.button("æ¤œç´¢å®Ÿè¡Œ"):
        st.session_state.mode = "execute"
    if col2.button("ä¿®æ­£ã™ã‚‹"):
        # ä¿®æ­£ãƒªã‚¯ã‚¨ã‚¹ãƒˆ â†’ è³ªå•ãƒ•ã‚§ãƒ¼ã‚ºã«æˆ»ã™
        st.session_state.mode = "question"
        st.session_state.chat_history = []
        st.session_state.proposal = None

# â”€â”€â”€ å®Ÿè¡Œãƒ•ã‚§ãƒ¼ã‚º â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def execute_phase():
    render_chat()
    # proposalï¼ˆJSONæ–‡å­—åˆ—ï¼‰ã‚’ãƒ‘ãƒ¼ã‚¹ã—ã¦æ¤œç´¢æ¡ä»¶ã«
    params = json.loads(st.session_state.proposal)
    query = utils.build_query(params)
    df = utils.search_patents(query)
    st.chat_message("assistant").write(f"ğŸ” æ¤œç´¢çµæœï¼š{len(df)} ä»¶ã§ã™ï¼")
    st.dataframe(df)
    csv_data = df.to_csv(index=False).encode("utf-8-sig")
    st.download_button("CSV ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰", csv_data, "results.csv", "text/csv")

# â”€â”€â”€ ãƒ¡ã‚¤ãƒ³ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.title("ğŸ” ç‰¹è¨±èª¿æŸ»æ”¯æ´ã‚·ã‚¹ãƒ†ãƒ ï¼ˆãƒãƒ£ãƒƒãƒˆUIç‰ˆï¼‰")

# ã‚¹ãƒ†ãƒ¼ãƒˆåˆæœŸåŒ–ï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒãƒ–ãƒ©ã‚¦ã‚¶ã‚’ãƒªãƒ­ãƒ¼ãƒ‰ã—ãŸã¨ãç”¨ï¼‰
if "mode" not in st.session_state:
    st.session_state.mode = "question"

if st.session_state.mode == "question":
    question_phase()
elif st.session_state.mode == "proposal":
    proposal_phase()
else:
    execute_phase()
